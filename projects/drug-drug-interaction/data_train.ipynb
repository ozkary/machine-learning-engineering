{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug to Drug Interaction (DDI) - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the ./data/ssp_interaction_type.csv.gz\n",
    "- Process the features\n",
    "  - Set the categorical features names\n",
    "  - Set the numeric features names  \n",
    "  - Set the target variable\n",
    "- Split the data\n",
    "  - train/validation/test split with 60%/20%/20% distribution.\n",
    "  - Random_state 42\n",
    "  - Use strategy = y to deal with the class imbalanced problem\n",
    "- Train the model\n",
    "  - LogisticRegression\n",
    "  - RandomForestClassifier\n",
    "  - XGBClassifier\n",
    "  - DecisionTreeClassifier\n",
    "- Evaluate the models and compare them\n",
    "  - accuracy_score\n",
    "  - precision_score\n",
    "  - recall_score\n",
    "  - f1_score\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191808 entries, 0 to 191807\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   ssp               191808 non-null  float64\n",
      " 1   interaction_type  191808 non-null  int64  \n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 2.9 MB\n",
      "None\n",
      "        ssp  interaction_type\n",
      "0  0.091837                 1\n",
      "1  0.093023                 1\n",
      "2  0.012346                 1\n",
      "3  0.069307                 1\n",
      "4  0.043103                 1\n"
     ]
    }
   ],
   "source": [
    "# open the csv file and read it into a pandas dataframe \n",
    "df = pd.read_csv('./data/ssp_interaction_type.csv.gz', compression='gzip')\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DDITrainData():\n",
    "    \"\"\"\n",
    "    Class to hold the training data for the DDI project\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, target_variable='interaction_type'):\n",
    "        self.df = df\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.target_variable = target_variable\n",
    "        self.categorical_features = None\n",
    "        self.numerical_features = None\n",
    "        # list of all features\n",
    "        self.all_features = None\n",
    "        \n",
    "    def process_features(self):\n",
    "        \"\"\"\n",
    "        Process the features for the model\n",
    "        \"\"\"        \n",
    "        # get the features\n",
    "        self.categorical_features = list(self.df.select_dtypes(include=['object']).columns)\n",
    "        self.numerical_features = list(self.df.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "        # remove the target feature from the list of numeric features\n",
    "        if self.target_variable in self.numerical_features:\n",
    "            self.numerical_features.remove(self.target_variable)\n",
    "\n",
    "        print('Categorical features',self.categorical_features)\n",
    "        print('Numerical features',self.numerical_features)\n",
    "        print('Target feature',self.target_variable)\n",
    "\n",
    "        # create a list of all features\n",
    "        self.all_features = self.categorical_features + self.numerical_features\n",
    "                \n",
    "        return self.categorical_features, self.numerical_features\n",
    "    \n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Split the data into training and validation sets\n",
    "        \"\"\"\n",
    "        # split the data in train/val/test sets, with 60%/20%/20% distribution with seed 1\n",
    "        X = self.df[self.all_features]\n",
    "        y = self.df[self.target_variable]\n",
    "        X_full_train, X_test, y_full_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "        # .25 splits the 80% train into 60% train and 20% val\n",
    "        X_train, X_val, y_train, y_val  = train_test_split(X_full_train, y_full_train, test_size=0.25, random_state=random_state)\n",
    "\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_val = X_val.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_val = y_val.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        # print the shape of all the data splits\n",
    "        print('X_train shape', X_train.shape)\n",
    "        print('X_val shape', X_val.shape)\n",
    "        print('X_test shape', X_test.shape)\n",
    "        print('y_train shape', y_train.shape)\n",
    "        print('y_val shape', y_val.shape)\n",
    "        print('y_test shape', y_test.shape)\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features []\n",
      "Numerical features ['ssp']\n",
      "Target feature interaction_type\n"
     ]
    }
   ],
   "source": [
    "# Process the features\n",
    "target_variable = 'interaction_type'\n",
    "\n",
    "# create an instance of the DDITrainData class to process the data\n",
    "train_data = DDITrainData(df, target_variable=target_variable)\n",
    "\n",
    "# get the features and target series\n",
    "cat_features, num_features = train_data.process_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (115084, 1)\n",
      "X_val shape (38362, 1)\n",
      "X_test shape (38362, 1)\n",
      "y_train shape (115084,)\n",
      "y_val shape (38362,)\n",
      "y_test shape (38362,)\n",
      "        ssp\n",
      "0  0.121622\n",
      "1  0.116279\n",
      "2  0.082353\n",
      "3  0.091954\n",
      "4  0.117647\n"
     ]
    }
   ],
   "source": [
    "# split the data in train/val/test sets\n",
    "# use 60%/20%/20% distribution with seed 1\n",
    "# use stratified sampling to ensure the distribution of the target feature is the same in all sets\n",
    "X_train, X_val, y_train, y_val, X_test, y_test = train_data.split_data(test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class DDIModelFactory():\n",
    "    \"\"\"\n",
    "    Factory class for DDI prediction model    \n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, categorical_features, numeric_features):\n",
    "        # Initialize the preprocessing transformers\n",
    "        self.scaler = StandardScaler()        \n",
    "        self.encoder = DictVectorizer(sparse=False)\n",
    "\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "        self.models = None\n",
    "        self.model = None\n",
    "\n",
    "    def preprocess_data(self, X, is_training=True):      \n",
    "        \"\"\"\n",
    "        Preprocess the data for training or validation\n",
    "        \"\"\"  \n",
    "        X_dict = X.to_dict(orient='records')\n",
    "        \n",
    "        if is_training:\n",
    "            X_std = self.encoder.fit_transform(X_dict)        \n",
    "        else:\n",
    "            X_std = self.encoder.transform(X_dict)\n",
    "            \n",
    "        print(f'Preprocess X shape {X.shape} training {is_training}')   \n",
    "        # Return the standardized features and target variable\n",
    "        return X_std\n",
    "    \n",
    "    def preprocess_target(self, y):\n",
    "        \"\"\"\n",
    "        Preprocess the target variable to make sure the data starts from 0 and is continuous\n",
    "        The target range starts at 1, so we need to subtract 1 from the target variable\n",
    "        \"\"\"\n",
    "        # encode the target variable\n",
    "        min = y.min()\n",
    "        max = y.max()\n",
    "        y_encoded = y\n",
    "        \n",
    "        if min != 0:\n",
    "            print('Min target value is not 0, encoding  y - 1')\n",
    "            y_encoded = y - 1\n",
    "\n",
    "        return y_encoded\n",
    "        \n",
    "    def train(self, X_train, y_train, reset=False, random_state=42, reg=10, estimators=100, iter=1000, depth=5):\n",
    "        \"\"\"\n",
    "         Train the models\n",
    "        \"\"\"        \n",
    "        if self.models is None or reset:\n",
    "            self.models = {\n",
    "                'logistic_regression': LogisticRegression(C=10, max_iter=iter, random_state=random_state, n_jobs=-1),\n",
    "                'random_forest': RandomForestClassifier(n_estimators=estimators, max_depth=5, random_state=random_state, n_jobs=-1),\n",
    "                'xgboost': XGBClassifier(n_estimators=estimators, max_depth=depth, random_state=random_state, n_jobs=-1),                \n",
    "                'decision_tree': DecisionTreeClassifier(max_depth=depth, random_state=random_state)\n",
    "            }\n",
    "        \n",
    "        for model in self.models.keys():\n",
    "            print('Training model', model)\n",
    "            self.models[model].fit(X_train, y_train)            \n",
    "\n",
    "    def evaluate(self, X_val, y_val, average='macro'):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the validation data set and return the metrics\n",
    "        \"\"\"\n",
    "\n",
    "        # create a dataframe to store the metrics\n",
    "        df_metrics = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'recall', 'f1', 'y_pred'])\n",
    "\n",
    "        # define the metrics to be calculated\n",
    "        fn_metrics = { 'accuracy': accuracy_score,'precision': precision_score,'recall': recall_score,'f1': f1_score}\n",
    "\n",
    "        # loop through the models and get its metrics\n",
    "        for model_name in self.models.keys():\n",
    "            \n",
    "            model = self.models[model_name]\n",
    "            y_pred = model.predict(X_val)\n",
    "                        \n",
    "            # add a new row to the dataframe for each model            \n",
    "            df_metrics.loc[len(df_metrics)] = [model_name, 0, 0, 0, 0, y_pred]\n",
    "\n",
    "            # get the row index\n",
    "            row_index = len(df_metrics)-1\n",
    "\n",
    "            # Evaluate the model metrics\n",
    "            for metric in fn_metrics.keys():\n",
    "\n",
    "                # determine which metrics call and use the corresponding average and zero_division parameters\n",
    "                score = 0\n",
    "                                \n",
    "                if metric == 'accuracy':\n",
    "                    score = fn_metrics[metric](y_val, y_pred)\n",
    "                elif metric == 'precision':\n",
    "                    score = fn_metrics[metric](y_val, y_pred, average=average, zero_division=0)                                \n",
    "                else:\n",
    "                    score = fn_metrics[metric](y_val, y_pred, average=average)\n",
    "                                \n",
    "                df_metrics.at[row_index,metric] = score\n",
    "           \n",
    "        return df_metrics\n",
    "\n",
    "    def save(model_name, path):\n",
    "        \"\"\"\n",
    "        Save the model\n",
    "        \"\"\"\n",
    "        # get the model from the models dictionary\n",
    "        model = self.models[model_name]\n",
    "\n",
    "        if model is None:\n",
    "            print('Model not found')\n",
    "            return\n",
    "            \n",
    "        # save the model\n",
    "        model.save(path)\n",
    "\n",
    "            \n",
    "    def predict(self, X_val):\n",
    "        \"\"\"\n",
    "        Predict the target variable on the validation data set and return the predictions\n",
    "        \"\"\"        \n",
    "        probs = self.model.predict_proba(X_val)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess X shape (115084, 1) training True\n",
      "Preprocess X shape (38362, 1) training False\n",
      "Min target value is not 0, encoding  y - 1\n"
     ]
    }
   ],
   "source": [
    "# hot encode the categorical features for the train data\n",
    "model_factory = DDIModelFactory(cat_features, num_features)\n",
    "X_train_std = model_factory.preprocess_data(X_train[cat_features + num_features], True)\n",
    "\n",
    "# hot encode the categorical features for the validation data\n",
    "X_val_std = model_factory.preprocess_data(X_val[cat_features + num_features], False)\n",
    "\n",
    "# preprocess the target variable\n",
    "y_train_encoded = model_factory.preprocess_target(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model logistic_regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model random_forest\n",
      "Training model xgboost\n",
      "Training model decision_tree\n"
     ]
    }
   ],
   "source": [
    "# train the models\n",
    "model_factory.train(X_train_std, y_train_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min target value is not 0, encoding  y - 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>0.314556</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.005630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.314608</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.005631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.315390</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.006608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.315234</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.005911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  precision    recall        f1\n",
       "0  logistic_regression  0.314556   0.003701  0.011763  0.005630\n",
       "1        random_forest  0.314608   0.003701  0.011765  0.005631\n",
       "2              xgboost  0.315390   0.008824  0.012044  0.006608\n",
       "3        decision_tree  0.315234   0.011737  0.011852  0.005911"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_val_encoded = model_factory.preprocess_target(y_val)\n",
    "df_metrics = model_factory.evaluate(X_val_std, y_val_encoded, average='macro')\n",
    "\n",
    "# print the metrics\n",
    "df_metrics[['model','accuracy', 'precision', 'recall', 'f1']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'device': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 5,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'multi_strategy': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine tune the model hyperparameters\n",
    "model_factory.models = None\n",
    "model_factory.train(X_train_std, y_train_encoded, estimators=100, iter=1000, depth=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
