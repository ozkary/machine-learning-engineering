{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Deep Learning\n",
    "\n",
    "In this homework, we'll build a model for predicting if we have an image of a bee or a wasp. For this, we will use the \"Bee or Wasp?\" dataset that was obtained from Kaggle and slightly rebuilt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "\n",
    "# !wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/bee-wasp-data/data.zip\n",
    "# unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The dataset contains around 2500 images of bees and around 2100 images of wasps.\n",
    "\n",
    "The dataset contains separate folders for training and test sets.\n",
    "\n",
    "## Model\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "- The shape for input should be (150, 150, 3)\n",
    "- Next, create a convolutional layer (Conv2D):\n",
    "  - Use 32 filters\n",
    "  - Kernel size should be (3, 3) (that's the size of the filter)\n",
    "  - Use 'relu' as activation\n",
    "- Reduce the size of the feature map with max pooling (MaxPooling2D)\n",
    " - Set the pooling size to (2, 2)\n",
    "- Turn the multi-dimensional result into vectors using a Flatten layer\n",
    "- Next, add a Dense layer with 64 neurons and 'relu' activation\n",
    "- Finally, create the Dense layer with 1 neuron - this will be the output\n",
    " - The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use SGD with the following parameters:\n",
    "- SGD(lr=0.002, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import History\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        # Convolutional layer\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        # MaxPooling layer\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Flatten layer\n",
    "        Flatten(),        \n",
    "        # Dense layer with 64 neurons and 'relu' activation\n",
    "        Dense(64, activation='relu'),\n",
    "        # Output layer with 1 neuron and 'sigmoid' activation for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "  \n",
    "    # Define the SGD optimizer with the specified parameters\n",
    "    optimizer = SGD(learning_rate=0.002, momentum=0.8)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and the defined optimizer\n",
    "    model.compile(optimizer=optimizer, \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Since we have a binary classification problem, what is the best loss function for us?\n",
    "\n",
    "- mean squared error\n",
    "- binary crossentropy\n",
    "- categorical crossentropy\n",
    "- cosine similarity\n",
    "\n",
    "For binary classification problems, the most commonly used loss function is binary crossentropy. Binary crossentropy is suitable when you have a binary outcome (e.g., 0 or 1) and is particularly well-suited for models outputting probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Question 2 - What's the number of parameters in the convolutional layer of our model? You can use the summary method for that.\n",
    "\n",
    "- 1\n",
    "- 65\n",
    "- 896\n",
    "- 11214912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 74, 74, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 175232)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11215873 (42.79 MB)\n",
      "Trainable params: 11215873 (42.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators and Training\n",
    "For the next two questions, use the following data generator for both train and test sets:\n",
    "\n",
    "\n",
    "```python\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "```\n",
    "\n",
    "-We don't need to do any additional pre-processing for the images.\n",
    "-When reading the data from train/test directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "-Use batch_size=20\n",
    "-Use shuffle=True for both training and test sets.\n",
    "\n",
    "For training use .fit() with the following params:\n",
    "\n",
    "```python\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - What is the median of training accuracy for all the epochs for this model?\n",
    "\n",
    "- 0.20\n",
    "- 0.40\n",
    "- 0.60\n",
    "- 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Epoch 1/10\n",
      "184/184 [==============================] - 18s 95ms/step - loss: 0.6730 - accuracy: 0.5592\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 17s 94ms/step - loss: 0.6346 - accuracy: 0.6220\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 18s 95ms/step - loss: 0.6022 - accuracy: 0.6726\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 18s 97ms/step - loss: 0.5500 - accuracy: 0.7158\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 17s 92ms/step - loss: 0.5172 - accuracy: 0.7558\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 17s 91ms/step - loss: 0.4817 - accuracy: 0.7811\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 18s 98ms/step - loss: 0.4526 - accuracy: 0.8009\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 18s 95ms/step - loss: 0.4243 - accuracy: 0.8240\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 17s 92ms/step - loss: 0.3977 - accuracy: 0.8284\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 18s 100ms/step - loss: 0.3625 - accuracy: 0.8637\n"
     ]
    }
   ],
   "source": [
    "# Create an ImageDataGenerator for both training and test sets\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def get_data_gen(datagen, path):\n",
    "    \"\"\"\n",
    "    Create an ImageDataGenerator and return the data generators for the training and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the data generators for the training set\n",
    "    generator = datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary',  # Use 'binary' for binary classification\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return generator\n",
    "\n",
    "# define the path to the training and test directories\n",
    "train_dir = './train'\n",
    "test_dir = './test'\n",
    "\n",
    "# Set up the data generators for the training set\n",
    "train_generator = get_data_gen(datagen, train_dir)\n",
    "    \n",
    "# Set up the data generators for test set\n",
    "test_generator = get_data_gen(datagen, test_dir)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Training Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Get the training accuracy values for each epoch\n",
    "training_accuracy_values = history.history['accuracy']\n",
    "\n",
    "# Calculate the median of training accuracy\n",
    "median_training_accuracy = round(np.median(training_accuracy_values),2)\n",
    "\n",
    "# Print the median training accuracy\n",
    "print(\"Median Training Accuracy:\", median_training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - What is the standard deviation of training loss for all the epochs for this model?\n",
    "\n",
    "- 0.031\n",
    "- 0.061\n",
    "- 0.091\n",
    "- 0.131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of Training Loss: 0.099\n"
     ]
    }
   ],
   "source": [
    "# Get the training loss values for each epoch\n",
    "training_loss_values = history.history['loss']\n",
    "\n",
    "# Calculate the standard deviation of training loss\n",
    "std_dev_training_loss = round(np.std(training_loss_values),3)\n",
    "\n",
    "# Print the standard deviation of training loss\n",
    "print(\"Standard Deviation of Training Loss:\", std_dev_training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations.\n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "- rotation_range=50,\n",
    "- width_shift_range=0.1,\n",
    "- height_shift_range=0.1,\n",
    "- zoom_range=0.1,\n",
    "- horizontal_flip=True,\n",
    "- fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an augmented ImageDataGenerator \n",
    "augmented_train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "augmented_train_generator = get_data_gen(augmented_train_datagen, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 31s 171ms/step - loss: 0.5308 - accuracy: 0.7460\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 29s 158ms/step - loss: 0.5033 - accuracy: 0.7664\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.4951 - accuracy: 0.7667\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 29s 159ms/step - loss: 0.4928 - accuracy: 0.7762\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 30s 162ms/step - loss: 0.4789 - accuracy: 0.7767\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 30s 162ms/step - loss: 0.4840 - accuracy: 0.7718\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 31s 169ms/step - loss: 0.4757 - accuracy: 0.7778\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 33s 181ms/step - loss: 0.4732 - accuracy: 0.7797\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 31s 167ms/step - loss: 0.4676 - accuracy: 0.7873\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 33s 180ms/step - loss: 0.4667 - accuracy: 0.7884\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the augmented data generator\n",
    "aug_history = model.fit(\n",
    "    augmented_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 - Let's train our model for 10 more epochs using the same code as previously.\n",
    "\n",
    "> Note: make sure you don't re-create the model - we want to continue training the model we already started training.\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "\n",
    "- 0.18\n",
    "- 0.48\n",
    "- 0.78\n",
    "- 0.108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the Test Loss for the Augmented Model: 0.49\n"
     ]
    }
   ],
   "source": [
    "# get the mean of all the epochs for the test loss for the augmented model\n",
    "aug_test_loss = round(np.mean(aug_history.history['loss']), 2)\n",
    "\n",
    "# print the mean of the test loss for the aumented model\n",
    "print(\"Mean of the Test Loss for the Augmented Model:\", aug_test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 - What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?\n",
    "\n",
    "- 0.38\n",
    "- 0.58\n",
    "- 0.78\n",
    "- 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the Test Accuracy for the Augmented Model: 0.78\n"
     ]
    }
   ],
   "source": [
    "# average of epochs 6 to 10 for the test accuracy for the augmented model \n",
    "aug_test_accuracy = round(np.mean(aug_history.history['accuracy'][5:]), 2)\n",
    "\n",
    "# print the average of epochs 6 to 10 for the test accuracy for the augmented model\n",
    "print(\"Average of the Test Accuracy for the Augmented Model:\", aug_test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
