{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Serverless Hosting\n",
    "\n",
    "Machine Learning models can be hosted on server less functions. To host this model, we must consider the size of the packages and their security settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file\n",
    "\n",
    "!wget https://github.com/alexeygrigorev/large-datasets/releases/download/wasps-bees/bees-wasps.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 12:51:50.728457: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-27 12:51:51.454574: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_keras_model(path, output_path):\n",
    "    \"\"\"\n",
    "    Convert a keras model to TensorFlow Lite format and save it to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = load_model(path)\n",
    "   \n",
    "    # Convert the model to TensorFlow Lite format   \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    lite_model = converter.convert()\n",
    "\n",
    "    # Save the converted model to a file\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(lite_model)\n",
    "        # display the byte size of the model\n",
    "        print(\"Size of the model(MB): \", round(len(lite_model) / 1024 / 1024, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 -  convert this model from Keras to TF-Lite format. What is the size?\n",
    "\n",
    "What's the size of the converted model?\n",
    "\n",
    "- 21 Mb\n",
    "- 43 Mb\n",
    "- 80 Mb\n",
    "- 164 Mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1x84toli/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1x84toli/assets\n",
      "2023-11-27 12:52:04.260632: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-11-27 12:52:04.260704: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-11-27 12:52:04.261411: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp1x84toli\n",
      "2023-11-27 12:52:04.262389: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-11-27 12:52:04.262405: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp1x84toli\n",
      "2023-11-27 12:52:04.264835: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-11-27 12:52:04.265460: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-11-27 12:52:04.468976: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp1x84toli\n",
      "2023-11-27 12:52:04.486043: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 224634 microseconds.\n",
      "2023-11-27 12:52:04.537382: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model(MB):  43.0\n"
     ]
    }
   ],
   "source": [
    "# convert the model\n",
    "convert_keras_model('./models/bees-wasps.h5', './models/bees-wasps.tflite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - What's the output index for this model?\n",
    "\n",
    "To be able to use this model, we need to know the index of the input and the index of the output.\n",
    "\n",
    "What's the output index for this model?\n",
    "\n",
    "- 3\n",
    "- 7\n",
    "- 13\n",
    "- 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing code\n",
    "\n",
    "# pip install pillow \n",
    "\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image  \n",
    "\n",
    "def download_image(url):\n",
    "    \"\"\"\n",
    "    download the image from the url and return a stream\n",
    "    \"\"\"\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    \n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    \"\"\"\n",
    "    Resize the image to target_size i.e. (150,150)\n",
    "    \"\"\"\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    convert the image to a numpy array and normalize it\n",
    "    \"\"\"\n",
    "    # convert to numpy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # convert to float32 to avoid overflow when multiplying by 255\n",
    "    img = img.astype('float32')\n",
    "    \n",
    "    # normalize to the range 0-1\n",
    "    img /= 255\n",
    "\n",
    "    return img  \n",
    "\n",
    "def load_lite_model(path):\n",
    "    \"\"\"\n",
    "    Load the TensorFlow Lite model and allocate tensors.\n",
    "    \"\"\"\n",
    "    # Load the TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()    \n",
    "\n",
    "    return interpreter, input_details, output_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details: 0\n",
      "\n",
      "Output details: 13\n"
     ]
    }
   ],
   "source": [
    "interpreter, input_details, output_details = load_lite_model('./models/bees-wasps.tflite')\n",
    "\n",
    "\n",
    "# Print details for input tensor(s)\n",
    "print(\"Input details:\", input_details[0]['index'])\n",
    "# Print details for output tensor(s)\n",
    "print(\"\\nOutput details:\", output_details[0]['index'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 -  what's the value in the first pixel, the R channel?\n",
    "\n",
    "Now we need to turn the image into numpy array and pre-process it.\n",
    "\n",
    "Tip: Check the previous homework. What was the pre-processing we did there?\n",
    "\n",
    "After the pre-processing, what's the value in the first pixel, the R channel?\n",
    "\n",
    "- 0.3450980\n",
    "- 0.5450980\n",
    "- 0.7450980\n",
    "- 0.9450980\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94509804\n"
     ]
    }
   ],
   "source": [
    "def download_and_preprocess_image(url, target_size):\n",
    "    \"\"\"\n",
    "    Download the image from the url, resize it to target_size and return a numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    img_stream = download_image(url)\n",
    "    img = prepare_image(img_stream, target_size)\n",
    "    img_normalized = preprocess_image(img)\n",
    "       \n",
    "    return img_normalized\n",
    "\n",
    "# download the image\n",
    "image_url = 'https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg'\n",
    "\n",
    "# convert the image to numpy array\n",
    "img_normalized = download_and_preprocess_image(image_url, (150, 150))\n",
    "\n",
    "# print the first pixel on the R channel (normalized)\n",
    "print(img_normalized[0, 0, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 -  What's the output of the model?\n",
    "\n",
    "Now let's apply this model to this image. What's the output of the model?\n",
    "\n",
    "- 0.258\n",
    "- 0.458\n",
    "- 0.658\n",
    "- 0.858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_inference(model_path, img_normalized):\n",
    "    \"\"\"\n",
    "    Load the TensorFlow Lite model and run the inference.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the lite model and the input/output details\n",
    "    interpreter, input_details, output_details = load_lite_model('./models/bees-wasps.tflite')\n",
    "\n",
    "    # print the input shape and type\n",
    "    print(input_details[0]['shape'])\n",
    "    print(input_details[0]['dtype'])\n",
    "\n",
    "    # print the output shape and type\n",
    "    print(output_details[0]['shape'])\n",
    "    print(output_details[0]['dtype'])\n",
    "\n",
    "    # set the input tensor with the normalized image\n",
    "    interpreter.set_tensor(input_details[0]['index'], [img_normalized])\n",
    "\n",
    "    # run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # get the output tensor\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    # get the result value\n",
    "    tensor_result = round(output_data[0][0],3)\n",
    "\n",
    "    # print the output\n",
    "    print('Tensor Output', tensor_result)\n",
    "\n",
    "    return output_data[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 150 150   3]\n",
      "<class 'numpy.float32'>\n",
      "[1 1]\n",
      "<class 'numpy.float32'>\n",
      "Tensor Output 0.659\n",
      "[0.6592137217521667]\n"
     ]
    }
   ],
   "source": [
    "# run the inference on a new image\n",
    "model_url = './models/bees-wasps.tflite'\n",
    "# run the inference\n",
    "result = img_inference(model_url, img_normalized)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the code and add the virtual environment\n",
    "\n",
    "- Create the virtual environment\n",
    "- Install the dependencies\n",
    "\n",
    "```bash\n",
    "pipenv shell\n",
    "pipenv install pillow tflite_runtime\n",
    "\n",
    "```\n",
    "- Use tensorflow lite instead of Tensorflow\n",
    "\n",
    "```bash\n",
    "import tflite_runtime.interpreter as tflite\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the notebook to a python file \n",
    "!jupyter nbconvert --to script homework.ipynb\n",
    "\n",
    "# rename the homework.py file to bees-wasps.py\n",
    "!mv homework.py bees_wasps.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 - What is the size of the base Docker image?\n",
    "\n",
    "Download the base image agrigorev/zoomcamp-bees-wasps:v2. You can easily make it by using docker pull command.\n",
    "\n",
    "So what's the size of this base image?\n",
    "\n",
    "- 162 Mb\n",
    "- 362 Mb\n",
    "- 662 Mb\n",
    "- 962 Mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agrigorev/zoomcamp-bees-wasps   v2             b9f6c13de368   4 days ago      662MB\n"
     ]
    }
   ],
   "source": [
    "!docker images | grep bees-wasps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 - What is the score of the image using the Docker instance?\n",
    "\n",
    "Now let's extend this docker image, install all the required libraries and add the code for lambda.\n",
    "\n",
    "You don't need to include the model in the image. It's already included. The name of the file with the model is bees-wasps-v2.tflite and it's in the current workdir in the image (see the Dockerfile above for the reference).\n",
    "\n",
    "Now run the container locally.\n",
    "\n",
    "Score this image: https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\n",
    "\n",
    "What's the output from the model?\n",
    "\n",
    "- 0.2453\n",
    "- 0.4453\n",
    "- 0.6453\n",
    "- 0.8453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker container result [0.4453350603580475]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# call the docker url with the image url localhost:8080\n",
    "\n",
    "url = 'http://localhost:8080/2015-03-31/functions/function/invocations'\n",
    "img_url = 'https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg'\n",
    "\n",
    "# call the docker url with the image url\n",
    "response = requests.post(url, data=json.dumps({'url': img_url})).json()\n",
    "print('docker container result',response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
