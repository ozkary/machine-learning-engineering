{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Serverless Hosting\n",
    "\n",
    "Machine Learning models can be hosted on server less functions. To host this model, we must consider the size of the packages and their security settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file\n",
    "\n",
    "!wget https://github.com/alexeygrigorev/large-datasets/releases/download/wasps-bees/bees-wasps.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_keras_model(path, output_path):\n",
    "    \"\"\"\n",
    "    Convert a keras model to TensorFlow Lite format and save it to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = load_model(path)\n",
    "   \n",
    "    # Convert the model to TensorFlow Lite format   \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    lite_model = converter.convert()\n",
    "\n",
    "    # Save the converted model to a file\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(lite_model)\n",
    "        # display the byte size of the model\n",
    "        print(\"Size of the model(MB): \", round(len(lite_model) / 1024 / 1024, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 -  convert this model from Keras to TF-Lite format. What is the size?\n",
    "\n",
    "What's the size of the converted model?\n",
    "\n",
    "- 21 Mb\n",
    "- 43 Mb\n",
    "- 80 Mb\n",
    "- 164 Mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphg9lkk78/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphg9lkk78/assets\n",
      "2023-11-21 16:19:19.750276: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-11-21 16:19:19.750374: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-11-21 16:19:19.750809: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmphg9lkk78\n",
      "2023-11-21 16:19:19.751855: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-11-21 16:19:19.751936: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmphg9lkk78\n",
      "2023-11-21 16:19:19.756514: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-11-21 16:19:19.843845: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmphg9lkk78\n",
      "2023-11-21 16:19:19.855642: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 104853 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model(MB):  43.0\n"
     ]
    }
   ],
   "source": [
    "# convert the model\n",
    "convert_keras_model('./models/bees-wasps.h5', './models/bees-wasps.tflite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - What's the output index for this model?\n",
    "\n",
    "To be able to use this model, we need to know the index of the input and the index of the output.\n",
    "\n",
    "What's the output index for this model?\n",
    "\n",
    "- 3\n",
    "- 7\n",
    "- 13\n",
    "- 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing code\n",
    "\n",
    "# pip install pillow \n",
    "\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image  \n",
    "\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    convert the image to a numpy array and normalize it\n",
    "    \"\"\"\n",
    "    # convert to numpy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # convert to float32 to avoid overflow when multiplying by 255\n",
    "    img = img.astype('float32')\n",
    "    \n",
    "    # normalize to the range 0-1\n",
    "    img /= 255\n",
    "\n",
    "    return img  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 -  what's the value in the first pixel, the R channel?\n",
    "\n",
    "Now we need to turn the image into numpy array and pre-process it.\n",
    "\n",
    "Tip: Check the previous homework. What was the pre-processing we did there?\n",
    "\n",
    "After the pre-processing, what's the value in the first pixel, the R channel?\n",
    "\n",
    "- 0.3450980\n",
    "- 0.5450980\n",
    "- 0.7450980\n",
    "- 0.9450980\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94509804\n"
     ]
    }
   ],
   "source": [
    "# download the image\n",
    "image_url = 'https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg'\n",
    "\n",
    "img_stream = download_image(image_url)\n",
    "img = prepare_image(img_stream, target_size=(150, 150))\n",
    "\n",
    "# convert the image to numpy array\n",
    "img_normalized = preprocess_image(img)\n",
    "\n",
    "# print the first pixel on the R channel (normalized)\n",
    "print(img_normalized[0, 0, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 -  What's the output of the model?\n",
    "\n",
    "Now let's apply this model to this image. What's the output of the model?\n",
    "\n",
    "- 0.258\n",
    "- 0.458\n",
    "- 0.658\n",
    "- 0.858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lite_model(path):\n",
    "    \"\"\"\n",
    "    Load the TensorFlow Lite model and allocate tensors.\n",
    "    \"\"\"\n",
    "    # Load the TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()    \n",
    "\n",
    "    return interpreter, input_details, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 150 150   3]\n",
      "<class 'numpy.float32'>\n",
      "[1 1]\n",
      "<class 'numpy.float32'>\n",
      "Tensor Output 0.659\n"
     ]
    }
   ],
   "source": [
    "# load the lite model and the input/output details\n",
    "interpreter, input_details, output_details = load_lite_model('./models/bees-wasps.tflite')\n",
    "\n",
    "# print the input shape and type\n",
    "print(input_details[0]['shape'])\n",
    "print(input_details[0]['dtype'])\n",
    "\n",
    "# print the output shape and type\n",
    "print(output_details[0]['shape'])\n",
    "print(output_details[0]['dtype'])\n",
    "\n",
    "# set the input tensor with the normalized image\n",
    "interpreter.set_tensor(input_details[0]['index'], [img_normalized])\n",
    "\n",
    "# run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# print the output\n",
    "print('Tensor Output',round(output_data[0][0],3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
